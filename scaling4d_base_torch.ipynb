{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22803e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1f225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from kauldron.modules import pos_embeddings\n",
    "from kauldron.modules import vit as kd_vit\n",
    "import mediapy\n",
    "from representations4d.models import model as model_lib\n",
    "from representations4d.models import readout\n",
    "import numpy as np\n",
    "from representations4d.utils import checkpoint_utils\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4921a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Hyperparameters\n",
    "model_patch_size = (2, 16, 16)\n",
    "im_size = (224, 224)\n",
    "model_size = \"B\"\n",
    "dtype = jnp.float32\n",
    "model_output_patch_size = (2, 8, 8)\n",
    "n_pixels_patch = (\n",
    "    model_output_patch_size[0]\n",
    "    * model_output_patch_size[1]\n",
    "    * model_output_patch_size[2]\n",
    ")\n",
    "num_input_frames = 16\n",
    "n_pixels_video = num_input_frames * im_size[0] * im_size[1]\n",
    "\n",
    "embedding_shape = (\n",
    "    num_input_frames // model_patch_size[0],\n",
    "    im_size[0] // model_patch_size[1],\n",
    "    im_size[1] // model_patch_size[2],\n",
    ")\n",
    "num_tokens = embedding_shape[0] * embedding_shape[1] * embedding_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40ec82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_ckpt_path = \"representations4d/scaling4d_dist_b.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27943763",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_state_dict = torch.load(\n",
    "    \"/Volumes/Storage/checkpoints/scaling4d_dist_b/encoder.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d99ea9",
   "metadata": {},
   "source": [
    "### Torch model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58ddb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from encoder import Encoder\n",
    "\n",
    "torch_encoder = Encoder(\n",
    "    input_size=(3, 16, 224, 224),\n",
    "    patch_size=(2, 16, 16),\n",
    "    num_heads=12,\n",
    "    num_layers=12,\n",
    "    hidden_size=768,\n",
    "    mlp_size=3072,\n",
    "    n_iter=1\n",
    ")\n",
    "torch_encoder.load_state_dict(encoder_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784321d",
   "metadata": {},
   "source": [
    "### JAX model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06c24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_depth_model = model_lib.Model(\n",
    "    encoder=model_lib.Tokenizer(\n",
    "        patch_embedding=model_lib.PatchEmbedding(\n",
    "            patch_size=model_patch_size,\n",
    "            num_features=kd_vit.VIT_SIZES[model_size][0],\n",
    "        ),\n",
    "        posenc=pos_embeddings.LearnedEmbedding(dtype=dtype),\n",
    "        posenc_axes=(-4, -3, -2),\n",
    "    ),\n",
    "    processor=model_lib.GeneralizedTransformer.from_variant_str(\n",
    "        variant_str=model_size,\n",
    "        dtype=dtype,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e076606",
   "metadata": {},
   "source": [
    "### Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eecd19f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = mediapy.read_video(\"representations4d/horsejump-high.mp4\")\n",
    "video = mediapy.resize_video(video, im_size) / 255.0\n",
    "video = video[jnp.newaxis, :num_input_frames].astype(jnp.float32)\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459cd2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-08-04 12:47:54,659:jax._src.xla_bridge:901: Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n",
      "WARNING:jax._src.xla_bridge:Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1754304474.659840  407062 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n",
      "I0000 00:00:1754304474.673495  407062 service.cc:145] XLA service 0x1196be560 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1754304474.673509  407062 service.cc:153]   StreamExecutor device (0): Metal, <undefined>\n",
      "I0000 00:00:1754304474.674765  407062 mps_client.cc:406] Using Simple allocator.\n",
      "I0000 00:00:1754304474.674778  407062 mps_client.cc:384] XLA backend will use up to 22906109952 bytes on device 0 for SimpleAllocator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(0)\n",
    "jax_params = jax_depth_model.init(key, video, is_training_property=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7ac2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_restored_params = checkpoint_utils.recover_tree(\n",
    "    checkpoint_utils.npload(jax_ckpt_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa8ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jax = (\n",
    "    jax_depth_model.apply(\n",
    "        jax_restored_params,\n",
    "        video,\n",
    "        is_training_property=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b782511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_torch = rearrange(\n",
    "    torch.from_numpy(jax.device_get(video).copy()),\n",
    "    \"b t h w c -> b c t h w\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ee61be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_torch = torch_encoder(video_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bde8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(output_jax) == len(output_torch)\n",
    "\n",
    "for (out_jax, out_torch) in zip(output_jax, output_torch):\n",
    "    assert torch.allclose(\n",
    "        out_torch,\n",
    "        torch.from_numpy(jax.device_get(out_jax).copy()),\n",
    "        atol=1e-3,\n",
    "        rtol=1e-3\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaling-4d-representations-QPYiXADD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
